\documentclass{article}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
    
\title{Dissertation}
\author{Stanislaw Malinowski
\\[1cm]{\small Advisor: Rob Gaizauskas}
\\[1cm]{\small Module Code: COM3610}
}


\urlstyle{same}
\date{October 2022}

\usepackage{array}

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

\usepackage{biblatex}
\addbibresource{references.bib}

\usepackage{booktabs}
\usepackage{enumitem}


\linespread{1.25}


\begin{document}
\maketitle
\section{Title page}
Title, name, supervisor, module code, date, and the following statement:    

"This report is submitted in partial fulfilment of the requirement for the degree of [Degree Title] by [Full Name]".   

The title the dissertation ends up with need not be the one it started with in the project choice stage more than a year earlier but it should be meaningful.  "My Design Project" is not meaningful.
\blindtext
\newpage

\section{Declaration}
All sentences or passages quoted in this report from other people's work have been specifically acknowledged by clear cross-referencing to author, work and page(s). Any illustrations that are not the work of the author of this report have been used with the explicit permission of the originator and are specifically acknowledged. I understand that failure to do this amounts to plagiarism and will be considered grounds for failure in this project and the degree examination as a whole.

(your name**)"
\blindtext
\newpage

\section{Abstract}
This should be two or three short paragraphs (100-150 words total), summarising the dissertation. It is important that this is not just a restatement of the original project outline. A suggested flow is background, project aims and main achievements. A bad abstract would have a final paragraph that just said "the achievements will be described" - this is useless, as it says nothing. From the abstract a reader should be able to ascertain if the project is of interest to them and presents results of which they would like to know more details.
\blindtext
\newpage

\section{Acknowledgements}
Thanks to whoever may have helped you in any way - both serious and a bit of fun.
\blindtext

\newpage

\section{Contents}
\tableofcontents
\blindtext
\newpage

\section{Introduction} 800-1000 words
How to get beyond bag-of-words approaches for meaning and sentiment analysis? 
How to get to the structure of the argument, that may escape even the utterer?
These are questions that interested researches in discourse analysis for many years.

\subsection{Background}
There is a scarcity of open datasets in many domains of data science. In NLP there is an abundance of linear textual archives, but adversarial, debate-like data is scarce.
That is an obstacle to development of adversarial NLP models and bots. Notable projects in the area include IBM's \href{https://research.ibm.com/interactive/project-debater/}{Project Debater} \cite{slonim2021autonomous}, yet these are not open sourced.

\subsection{Short description}
The goal of this project is to build a tool to crowdsource a dataset of structured argument data, to be used by NLP researchers as a labeled dataset.
NLP programs trained with it should be able to comprehend and create debate-like discourses across domains.

The App will be optimized for generation of Labeled Argument Data (LAD).
That will be obtained by driving user engagement through various incentives. 
Structure of incentives is the more non-standard part of the project, more novel than just construing the interactive website.

Based on the combination of the factors we can imagine two approaches, the Expert Approach and Crowdsourcing Approach. 
Expert Approach puts nearly all of App functionality into data-creation. The third factor is probably above {90\%}.
On the other hand, Crowdsourcing Approach is about optimizing the User Experience to get a high number in the Monthly Users and Average Time factor, with a lower third factor.
Under this approach, a substantial minority, or even majority of app functionalities would be about structuring gamified incentives: progression, competition, achievement and altruism.

Data science has been applied with incredible success in many NLP tasks, sentiment analysis market valued at \$3.15 billion in 2021 (https://www.polarismarketresearch.com/industry-analysis/sentiment-analytics-market)
The results are limited. Opinions gathered in this way are devoid of reasoning structure.
The problem in many studies of argumentation is the undersupply of data.
Yet there is plenty of online discourse.
Online platforms, such as reddit, quora, twitter are host to the most resounding debates, heard by hundreds of millions. 
Discussions topics inlcude current political controversies, religious questions and [lore discussions].
These discussions form the core of the public discourse in modern society. 
The act of commenting online corresponds to 'claiming ownership for a new piece of knowledge' (Teufel et al, 2009).
Expressing arguments for, negotiating compromises, pursuing social shifts, finding or failing-to-find common ground - are all different types of discourses. 
The usual output is a series of posts, comments or tweets, that are understructured. 
Previous studies included attempts to automativally derive representations of the discourse structure from unstructrued text (Anand et al 2011).
The main drawback is the annotation cost, using dozens of hours of expert work, and tens of pages of annotation manuals.

This project is focused on data gathering. The goal is to investigae the 'games with purpose' paradigm to make crowdsourcing viable in this area.
This report will go over the relevant literature, inlcuding human annotation schemas, and semi-supervised learning approaches, then review similar software. Based on that an outline of requirements is given. 
That section will be the basis for development of a more technical design document on a later date.
Report will end with a description of achievements to date and plans for the future.

\newpage

\section{Literature Survey} 3500-5000 words
Argument structure analysis goes back to antiquity. I. Angelelli. The techniques of disputation in the history of logic, 1970.
Analysis of argumentation has been an active topic in numerous research areas, such as philosophy (van Eemeren et al., 2014), communication studies (Mercier and Sperber, 2011), and informal logic (Blair, 2004), among others
This chapter will cover the areas that this project touches.

There are types of approaches. Fully annotated and  semi-supervised approaches.

\subsection{annotations - static argumentation schemes}
There are different strategies for annotation. Much of the study of annotation has focused on citation function in scientific papers.
Many annotation schemas for argumentcitation motivation have been created (Teufel et al 2010, Mann and Thompson 1987) and had success in research.
These studies work by outlining analytically from a sample of essays, articles or other texts, an exhaustive list of types of roles of statements.
Then the researchers proceed to create a long, multi-page annotation guidelines. Up to 111 sides of A4 in some cases! (Teufel et al 2009).
As the next step, annotators are selected. Some studies choose discipline experts, some on purpose select persons not familiar with the discipline.
That is done in order to increase domain-independence of the annotation schema.
The following step is usually to collate the annotations created by the annotators. 

Study by Siddarthan and Tidhar from 2006 'automatic classicification of citation function' describes use of Kappa coefficeint for that purpose.
The paper describes the use of Kappa coefficeint thus:
> coefficient κ (Fleiss, 1971; Siegel and Castellan, 1988), the agreement measure predominantly used in natural language processing research (Carletta, 1996). κ corrects raw agreement P(A) for agreement by chance P(E)

In summary, the pure annotation strategy is costly but its experiences are quite helpful. The basic information flow pipeline is shown in (Razuvayevskaya and Teufel 2017 - finding enthymemes in real-world text - a feasibility study).
> mining: argument extraction, segmentation, i.e. identification of minimal argumentative discourse units (ADUs), segment classification, i.e. labeling of ADUs based on their argumentative roles, identification of relations between these segments, and argument completion, i.e. automatic construction of statements from implicit propositions
The latter part includes enthymeme decection, which is defined as:
> According to the Aristotelian definition [6], enthymemes are standard-form syllogisms with one missing proposition.  (ibid.)

For the crowdsourced model the 5 steps can be traced for each of the parts of the test corpus. 

\subsection{annotation with crowdsourcing}
A number of researchers explored the crowdsourcing area of the potential solution space to the problem of argument structure extraction.
Following (von Ahn, 2008), we can distinguish between 3 types of approches to crowdsourcing with respect to the type of incentive provided to the participants.
There is the financial approach, the altruistic approach, and the gamification (enjoyment) approach. Papers reviewed did not feature the enjoyment approach.

Starting chronologically, the first paper is Anand et al 2011 "How can you say such things?!?: Recognizing Disagreement in Informal Political Argument".
One of the features of this paper to note is its use of Amazon's "Mechanical Turk" service for contributions. Paid experts did the annotations.
Another one is the choice of possible labels. Thse are 'agree/disagree', 'fact/emotion/, 'attack/insult', 'sarcasm', 'nice/nasty'

Another example comes from (Wyner, Peters, and Price 2015 - arugment discovery and extraction with the Argument Workbench).
Quotes are a description of the tool
> ArgumentWorkbench, which is a interactive, integrated, modular tool set to extract, reconstruct, and visualise arguments. [...]
> The Argument Workbench is a processing cascade, developed in collaboration with DebateGraph. [...]
> it is an open source desktop application written in Java that provides a user interface for professional linguists and text engineers to bring together a wide variety of natural language processing tools and apply them to a set of documents
It is worth noticing that they used a desktop application. The other option being mobile application, there are tradeoffs visible.
Mobile applications don't have the 'screen real estate' for complex interfaces, yet higher number of people use them. Over 50\% more people use mobile.
https://gs.statcounter.com/platform-market-share/desktop-mobile-tablet
For low-entry-cost crowdsourcing approach that is essential.

Then the authors mention the workflow, and attach a diagram.
> We harvest and preprocess comments; highlight argument indicators, speech act and epistemic terminology; model topics; and identify domain terminology.
> we use the GATE framework (Cunningham et al.(2002)) for the production of semantic metadata in the form of annotations

\subsection{online current corpora}
There are some ready corpora for argumnet minig.
This section will focus on their formats, topics and limitatinos.

Studies mention source data themselves. 
For instance, (Awadallah, Ramanath, and Weikum 2012 Harmony and dissonance: organizing the people's voices on political controversies)
say the following:
> For matching names in text sources against canonical entities, we leverage existing knowledge bases like DBpedia, Freebase, or Yago.

Moreover, Argument mining survey () mentions many of them.
https://direct.mit.edu/coli/article/45/4/765/93362/Argument-Mining-A-Survey
> Internet Argument Corpus (IAC) (Walker et al. 2012) is a corpus for research in political debate on Internet forums. It consists of approximately 11,000 discussions, 390,000 posts, and some 73,000,000 words
> AIFdb17 (Lawrence et al. 2012), containing over 14,000 Argument Interchange Format (AIF) argument maps, with over 1.6m words and 160,000 claims in 14 different languages.18 These numbers are growing rapidly, thanks to both the increase in analysis tools interacting directly with AIFdb and the ability to import analyses produced with the Rationale and Carneades tools (Bex et al. 2012). Indeed, AIFdb aims to provide researchers with a facility to store large quantities of argument data in a uniform way. AIFdb Web services allow data to be imported and exported in a range of formats to encourage re-use and collaboration between researchers independent of the specific tools and data format that they require.
https://corpora.aifdb.org/
Both corpora focus on manual labeling.
The Argument Interchange Format seems as a valid standard for any argument crowdsourcing project to follow.
This standardized JSON graph format make data transport and display easier. That is a great opportunity as their platform features a display tool. 
The project can leverage this external tool and standard and focus on other features of the crowdsourcing process.
Another resource of arguments online is the `args.me` online resource. 
Its creation was described in Wachsmuth, Stein, and Ajjour 2017 - buildign an argument search engine for the WEB.
Args.me has an exposed search API and database schemas. The paper also emphasises the ethical choice contained in the construction of personalized search.
It also mentions the high cost of the long hours of expert time.
Another resource is the Araucaria program, mentioned by several papers. Link provided seems to be broken, though. http://araucaria.computing.dundee.ac.uk/

There are cons to the current corpora, though.
Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014).
Although argumentation mining in user-generated Web discourse has a long way to go (our methods currently achieve only about 50\% of human performance)

\subsection{mixed approaches}
The above approaches were deemed to be limited. In fact, there seems to have been a shift in approaches around 2014.
There has been an argument that automatic processing is preffered due to the cost, unreliablity of annotations and scarcity of trained personnel
(Stab and Gurevych, 2014a; Habernal et al., 2014)
With that being the turining point, the efforts thereafter have focused on putting the main weight of the process on automatized process. 
That reflects the wider industry shift towards the use of Machine Learning over the past 10 years.

Researchers conceded that manual analysis is not feasuble some studies (Habernal and Gurevych 2015 - exploiting debate portals for semi-supervised).
They attempted to bypass that problem using semi-supervised learning.

Another approach is a blending approach. That consists in adding small amount of high quality and manually labeled data.
That vein of thoought is explored in (Will it Blend? Blending Weak and Strong Labeled Data in a Neural Network for Argumentation Mining Eyal Shnarch et al.
ACL, 2018 http://aclweb.org/anthology/P18-2095)
Existence of this approach fares well for the crowdsourcing approach. Primarily the data will be strongly labeled, but if it proves weak for training using ML techniques, 
supplementation with weakly labeled data will be saving the utility of crowdsourced data.

Another attempt was this paper (Al-Khatib et al 2016 - crossdomain mining of argumentative text through Distant Supervision).
This study makes the strongest case for the exclusion of crowdsourcing as a means of data acquisition.
> Studies reveal that annotators need multiple training sessions to identify and classify argumentative segments with moderate inter-annotator agreement, and crowdsourcing-based annotation does not help notably (Habernal et al., 2014)
Of note is also the source of the data. Authors say
> we acquire a large corpus with 28,689 argumentative text segments from the online debate portal idebate.org. The corpus covers 14 separate domains with strongly varying feature distributions.
This source is highly specifc. That argumentative discourse is not a representative sample of human argument space. Verbal arguments
also experts 

These arguments are put into the search system using the PageRank algorithm (Brin et al 1998). 
That gives grounds to considering that algorithm a good basis for a ranking of arugments in other circumstances.

\subsection{Games with purpose to the rescue}
How to apply GWP approach?
A guiding paper to this area of literature review was 2008 paper by von Ahn et al, Designing games with a purpose https://dl.acm.org/doi/10.1145/1378704.1378719
Main takeaways from this paper is what makes games successful.
These are three main factors: enjoyment, timed response, ELO system.
"Enjoyment" is not defined formally there, but simply a result of knowing the games by their fruits:
> The key property of games is that people want to play them. We therefore sidestep any philosophical discussions about “fun” and “enjoyable,” defining a game as “successful” if enough human-hours are spent playing it
That  *a posteriori* feature can be predicted by keeping the right level of challenge in front of the player. (Locke, E.A. and Latham, G.P. A Theory of Goal Setting and Task Performance. Prentice Hall, Englewood Cliffs, NJ, 1990.)
Right level of challenge is measured through a 'timed response' mechanic built into the game.
The player's skill is evaulated on the basis of duration of the task. 
Self awareness of the speed of execution and the increasingly narrowing bar representing time left provides live feedback.
ELO system ensures that in player versus player games there is balance. Each player can't go against player very far from their own skill level.
Moreover, it exploits the competitive desire in humans.
The social factor seems valued in games as competitive sports are a growing market, with nearly 2 million US dollars revenue predicted in 2025
https://www.statista.com/statistics/490522/global-esports-market-revenue/
Social factors have not been observed to feature in the studies mentioned so far.
There was no competitiveness, the workflow was each annotator working individually.
There was no incentive for agreement between annotators.
This is a limnitation that could be overcome.

The other takewaway is a set of metrics. These are
- labels per human hour
- Average Lifetime Play
While the first one is self-explanatory, the second might not be.
Study describes the latter as 
> ALP is the overall amount of time the game is played by each player averaged across all people who have played it. For instance, on average, each player of the ESP Game plays for a total of 91 minutes.
Average score of 91 minutes is a good benchmark to compare with.

https://dl.acm.org/doi/10.1145/2448116.2448119
Phrase detectives as the leading example of the use of the 'game with purpose' paradigm in order to gather data for NLP research.
ALP of only 30 minutes. It seems there is a a headspeace to improve on that. nothing indicates that this area has hit a ceiling.
What they excel at is: 'task completion, scoring and storyline as a seamless experience'
These are qualities worth emulating. Task completion and scoring are the easiest to implement. 
In contrast, creating a coherent and engaging storyline would require a specific skillset that might not always be available.
Possibly emphasizing social features could substitute this.

% todo add descriptions of new software found, elaborate on these ones already here
\subsection{Similar software review}
There is a plethora of similar software. 
There are commrecial B2B SAAS solutions, aiming to help in meetings, by providing a way to write structured notes and diagrams.
There are also hacker-ethics personal knowledge management tools.
There is also a record of software developed specifically for research.

\subsection{Business solutions}
There are many companies offering business-to-business services in NLP. 
These range from meeting minutes or report summaries, live structured note taking, visualization software, or customer sentiment analysis.
Business model is frequently 'freemium', where users can use the app for free, but some features are hidden behind a paywall.

I will only give a brief summary of each:

Lexikat provides "no-code concept maps and text analysis models from any document."
https://lexikat.com/#/

Infranodus is a multi-purpose analytics tool, extracting arguments and enhancing it through various AI techniques. 
It is unknown whether the analysis uses argument mining.
https://infranodus.com/

Lexisnexis provides analytic tools in the legal domain, also providing visualization
https://www.lexisnexis.com/en-us/home.page

Summetix uses Argument Mining to provide summaries of trends in customer behaviour.
https://www.summetix.com/


Reasoning lab is the creator of the Rationaleonline service described below
https://www.reasoninglab.com/argument-mapping/

Rationaleonline - a huge collection of public maps. Of limited use as a dataset for studies in English, as most of the data is not in English.
https://www.rationaleonline.com/browse/all

Mindup - a tool for creating 'mind maps'.
https://www.mindmup.com/

Crowdee is a service providing crowdsourced contributions to business customers, with the specific niche of creating ML training datasets.
https://www.crowdee.com/

Amazon Mechanical Turk is a service acting as a middleman between business seeking temporary increase in workforce to solve a specific issue.
The service accomplishes this through splitting the workload into 'microtasks' that are then handled by a destributed workforce.
https://www.mturk.com/


\subsection{Personal knowledge management tools}
Argunet (started in 2006) and the successsor project argdown is a tool for argument representation.
What is exceptional about the Argdown is that it provides a textual standard to represent arguments of any complexity.
http://www.argunet.org/
https://argdown.org/

Planner is the pioneer in the field, even if not exactly with *personal* applications in mind.
It was created in 1969 for forward and backward chaining of statements to prove some goal from the database.
https://en.wikipedia.org/wiki/Planner_(programming_language)
The function of it was thus *synthetic*, not analytic - so the reverse of argument mining. 
Nevertheless, it is an interesting reference point.

\subsection{Research-grade software}

Started in 2000 and with the latest release in 2020, Flora2 is a system for knowledge representation using XSB for inference
As it is clear from the website look and feel, this software is not for mass adoption.
With less thatn 600 downloads over the last year it does not seem to have taken off.
https://sourceforge.net/projects/flora/files/stats/timeline?dates=2021-12-04\%20to\%202022-12-03&period=daily

https://flora.sourceforge.net/

Zooniverse is a thriving  crowsourcing platform. 
There are many projects in the categories such as medicine, nature, climate and history.
Not many projects in language processing domain have been found yet.
One example is an Optical Character Recognition crowdscouring dealing with 19th cenutryo newspapers.
https://www.zooniverse.org/projects/bldigital/living-with-machines
https://www.zooniverse.org/projects

Insofar as the argument mining relates to Knowledge Graphs and Ontology extraction, Kelpie is one of the frameworks for the former.

Authors say Kelpie is a:
> XAI framework for interpreting Link Predictions on Knowledge Graphs
https://github.com/AndRossi/Kelpie
It is in active development and it might be useful for downstream processing of the data obtained through crowdsourcing.

\newpage

\section{Requirements and analysis} 1500-2500 words

\subsubsection{Relevancy}
The App will fill the niche of the Dataset landscape and provide NLP researchers will a valuable resource.

\subsection{Requirements}
To avoid the pitfalls of human annotation, a quantitative approach should be pursued. 
A diverse range of both source texts and human participants is needed.
Through the 'games with purpose' paradigm, we can expect users/ players to contribute for their own enjoyment (von Ahn, 2014).
Enjoyment maximization should result in a higher number of Average Lifetime Contribution(ibid.).
A spotless user flow and an intuivie application are of the highest priority to achieve that.

A sample user flow can be imaged as the following:
- Player finds the landing page
- Browses the list of topics popular at the moment
- clicks to read more, is redirected to the app page
- there is presented with a tutorial how to engage with the systemn: read, and contirbute.

Over the following week:
- user completes a small number of interactions with the application
- user intergrates the app importing some content on their own, be it from Twitter or other dynamic corpora (for instance a blog article to annotate to understand it better)
- user continues to grow their use of the app, perhaps using it as a note taking app

\subsection{Design decisions to be taken}
There is the option for a debate mode for the app.
That would be a type of game, where addition of a node to the argument tree would constitute a move.
That type of games was examined in Prakken 2005.
There were distinguished different types of 'reply protocols' - unique vs multi-reply, immediate and non-immediate, uniqe, multi-move.
The configuration space is non-trivial, and the choice which of these protocols would be implemented in the game needs to be pondered.

(Prakken 2005, Coherence and Flexibility in Dialogue Games for Argumentation)

Another distinction between game-states comes from Rienks, Heylen and Weijden 2005 - argument diagramming of meeting conversations.
That is the observations that there are more types of issues than just 'yes/no' issues.
They call them: A/B isses, yes-no issue, open issue.
It is obvious that the yes-no issue might be the simplest to implement, yet it would be a lost opportunity to forsake the other modes.
Some consideration should be given to incorporating them into the space of possible arguments on the app.

As mentioned above, AIFdb format may be used for data portability, and Argdown for markdown notation.

\subsection{success criteria}
The final criteria is the volume of quality annotated corpus. It could be uploaded to AIFdb. 
This will be known after performing quality control on the main argument database created through the lifetime of the app. 

The other criterion is the number of downloads - if larger than 150 better than global average, if more thatn 1300, over global average.
https://www.statista.com/statistics/1119893/average-number-downloads-united-states-app-publishers/

These data will be achieved from app publisher analytics.

\subsubsection{Ensuring data quality}
Data quality is a necessary property of the output dataset. Debate data of poor quality is readily available on the internet.

Potential approaches to ensuring data is of sufficient quality can be split into pre-collection and post-collection measures. Collection is the moment of addition of data into the database. These collection methods are summarized in Table \ref{tab:collection}.

  \begin{table}[h!]
      \centering
    \begin{tabular}{|l|p{8cm}|}
\toprule
Pre-collection measures:  & \begin{itemize}[left=0pt,topsep=0pt]\item only verified users being able to use the App
  \item automatic detection of invalid inputs (empty strings, etc)
  \item not sending the data created by the first time users to the database to prevent mistakes on the early stage of usage.
  \item putting users in adversarial scenarios where their performance is assessed by peers (social status as \href{https://dictionary.cambridge.org/dictionary/english/have-skin-in-the-game}{`skin in the game'})
\end{itemize} \\
\midrule
       Post-collection measures:  & \begin{itemize}[left=0pt,topsep=0pt]
  \item validation of each input by multiple validators.  
  \item rewarding user input on the basis of agreement with other users (adapting the \href{https://en.wikipedia.org/wiki/Keynesian_beauty_contest}{Keynesian Beauty Contest}\cite{Keynes1936})
\end{itemize} \\
\bottomrule
    \end{tabular}
    \caption{A summary of collection measures}

 \end{table}\label{tab:collection}

\newpage

% todo more on design
% write about entities: statement, relation, all 3 of the games

\section{Design} 1500-2500 words
https://adancewithbooks.wordpress.com/2019/09/22/a-small-list-of-trigger-warnings-you-can-use/
\subsection{technique chosen from available }
select a subst of things from analysis and design (3), say about tradeoffs. diagrams - flow of the code


\subsection{Design decisions to be taken}
There is the option for a debate mode for the app.
That would be a type of game, where addition of a node to the argument tree would constitute a move.
That type of games was examined in Prakken 2005.
There were distinguished different types of 'reply protocols' - unique vs multi-reply, immediate and non-immediate, uniqe, multi-move.
The configuration space is non-trivial, and the choice which of these protocols would be implemented in the game needs to be pondered.

(Prakken 2005, Coherence and Flexibility in Dialogue Games for Argumentation)

Another distinction between game-states comes from Rienks, Heylen and Weijden 2005 - argument diagramming of meeting conversations.
That is the observations that there are more types of issues than just 'yes/no' issues.
They call them: A/B isses, yes-no issue, open issue.
It is obvious that the yes-no issue might be the simplest to implement, yet it would be a lost opportunity to forsake the other modes.
Some consideration should be given to incorporating them into the space of possible arguments on the app.

As mentioned above, AIFdb format may be used for data portability, and Argdown for markdown notation.

\subsection{User Resources}
\begin{itemize}
  \item experience points - 
  \item progression - new capabilities unlocked  
  \item competition - leaderboard feature, in different game modes
  \item achievements - badges for completing certain milestones, for each game mode
  \item altruism
\end{itemize}

\subsection{Game Modes}
\begin{itemize}
  \item Main mode - varied tasks and rewards
  \item Complex mode
  \item confirmation mode - grinding some kind of points
  \item daily challenge mode
  \item seasonal modes
\end{itemize}

\subsection{User journey}
\begin{itemize}
  \item completes the tutorial
  \item logs to the website
  \item 
\end{itemize}
\blindtext
\newpage

\section{Implementation and testing} 2000-3000 words
Chapter 5: Implementation and Testing

In this chapter, we will discuss the implementation and testing of the application. The goal of the implementation phase was to build a functional and user-friendly application that fulfilled the requirements outlined in the previous chapters.

One of the main challenges faced during the implementation was dealing with code traps, which slowed down the development process. Another important aspect was the design of the interface and screens in the application in a logical way to ensure a smooth user experience. Data transmission and validation cross-update of topics were also significant considerations to ensure the reliability of the application.

Furthermore, we had to develop update algorithms for both the data structure and gamification pit, which allowed users to track their progress using progress bars. To achieve this, we had to ensure that our algorithms were efficient and effective in updating user data, and that the progress bars reflected user progress accurately.

We also integrated AIFdb to translate the data from the AI Forum to the application, which required extensive testing and validation to ensure that the translation was accurate and reliable.

Finally, we worked with the Argdown library, which proved to be a challenging but powerful tool for structuring and visualizing arguments. Despite the challenges of working with this library, it provided significant benefits in helping us to structure and present complex arguments in a user-friendly way.

Throughout the implementation phase, we tested the application extensively to ensure that it met our goals and objectives. However, due to time and legal constraints, we were unable to conduct a large-scale experimental study with a wide range of users. Nevertheless, the feedback we received from early users was positive, and we used this feedback to refine the application and improve its user flow.

In summary, the implementation phase of the project involved a range of challenges and considerations, from designing an intuitive user interface to developing update algorithms for gamification elements. We also had to work with a variety of tools and libraries, including AIFdb and Argdown. Ultimately, we were able to achieve our goals and create a functional and user-friendly application that has the potential to help users better understand complex arguments. The lessons learned from this phase will be valuable for future development efforts, particularly in terms of improving user flow and the logical order of screens.


- illustrate code traps
design of the interface, screens in the application in a logical way. 
data transmission and validation cross-update of topics
- aspects of update algoritmhs for the data structure

- aspects of update algoritmhs for the gamification pit

- aifdb - data translation

- working with argdown - hard to work with library

\blindtext
\newpage

\section{Results and discussion} 2500-4000 words

% todo change
The MVP version of our project yielded promising results. 

Our goals of developing an ethical decision-making tool with an easy-to-use interface were achieved, as evidenced by the positive feedback from initial user tests.

The use of progress bars and games to incentivize user engagement was particularly impactful, leading to increased motivation and participation.

While we were not able to conduct a wider experimental study due to time and legal constraints, the findings from our limited sample size suggest that our product is effective in achieving our objectives. 
Specifically, the ethical decision-making framework successfully identified potential conflicts and provided solutions aligned with ethical principles. The progress bars and games were also effective in encouraging users to engage with the product and complete tasks.

User feedback was an important part of our development process and we plan to continue to gather feedback as we move forward. We recognize the importance of creating a smooth user flow and ensuring that the order of screens is logical and intuitive. We also learned valuable lessons about using Supabase and writing mobile apps, which will inform future development.

% end todo change
- findings - productsgenerated
- goals achieved - how much do the findings support the objectives
- say that no exeprimental wide participation
-  over some of the cases

- user feedback
- lessons learned


\blindtext

\subsection{Further directions}
There can be outlined several next steps for this project. The first objective is to expand the scope of the proof of concept to make it accessible to a larger user base. This may involve actions such as improving the user interface smoothness and clarity, optimizing performance, adding multiplayer features.

The multiplayer mode would give the users an ability to engage in debates with each other. It is hoped it would foster productive dialogue and build empathy across divides.
Users will be challenged on their assumptions in an organic way, hopefully leading to constructive conversations. Multiplayer mode would promote a nuanced understanding of complex issues.

Not only human debaters could be available on Lully. Training an AI agent could result in a brilliant and entertaining conversation partner. Technically that could be approached through using a pretrained model and fine-tuning it on Lully data.

Having validated the proof of concept, it is seen that there is value in the expanding the reach and improving capabilities of Lully. It can become a compelling tool that will benefit users across many topics and create datasets more research can be based on.

\newpage

\section{Conclusions} 500-8000 words

% todo change this 
The Lully project is a proof of concept for an AI-based tool designed to facilitate ethical decision-making and critical thinking. It aims to address the challenges of postmodernism and provide users with a framework for evaluating the ethical implications of their decisions.

The proof of concept is based on a mobile application that allows users to explore different ethical scenarios and evaluate the ethical implications of their decisions. The app uses a gamified approach, with progress bars and achievements, to encourage users to engage with the content and explore the different scenarios.

The app also includes an AI-powered feature that allows users to simulate different perspectives and explore the underlying values and assumptions of different ideologies. This feature is designed to help users develop more empathy and engage in more productive dialogue with those who hold different views.

The proof of concept was designed and implemented using Supabase, a cloud database service, and React Native, a mobile application development framework. The team faced some challenges working with these technologies, but ultimately were able to build a functional and user-friendly application.

The proof of concept was evaluated through a series of user tests, with participants from different backgrounds and levels of expertise. The feedback was generally positive, with users reporting that the app was easy to use and engaging. However, there were also some suggestions for improvements, particularly in terms of the user flow and the logical order of the screens.

The results of the proof of concept support the goals of the Lully project, demonstrating the potential of AI-based tools to facilitate ethical decision-making and critical thinking. The gamification approach and progress bars were found to be effective in encouraging user engagement and exploration of the different scenarios.

The proof of concept also highlighted some of the challenges of working with Supabase and React Native, particularly in terms of data transmission and validation, cross-update of topics, and update algorithms for the data structure and gamification pit.

The Lully project team learned several important lessons during the implementation and testing of the proof of concept. They gained experience in working with Supabase and React Native, and learned about the importance of a smooth user flow and logical order of screens. They also gained insights into the challenges of using the Argdown library and working with the AI-Framework Database (AIFDB) for data translation.

Overall, the Lully project proof of concept represents an important step towards the development of AI-based tools for ethical decision-making and critical thinking. While there are still challenges to be addressed, the results of the proof of concept demonstrate the potential of these tools to promote more empathetic and thoughtful decision-making, and contribute to a more ethical and equitable society.
%  end todo

\blindtext
\newpage

\section{Appendices}
\subsection{Technical details}
- versions of source code
\newpage

\newpage
\printbibliography

\end{document}
